{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ebf220-b9f5-4bab-a926-4baf6f93d5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported\n",
      "âœ“ Paths configured\n",
      "âœ“ Data loaded: 11 train, 2 val, 3 test\n",
      "Columns in train data: ['mfcc_0_mean', 'mfcc_0_std', 'mfcc_0_min', 'mfcc_0_max', 'mfcc_1_mean', 'mfcc_1_std', 'mfcc_1_min', 'mfcc_1_max', 'mfcc_2_mean', 'mfcc_2_std', 'mfcc_2_min', 'mfcc_2_max', 'mfcc_3_mean', 'mfcc_3_std', 'mfcc_3_min', 'mfcc_3_max', 'mfcc_4_mean', 'mfcc_4_std', 'mfcc_4_min', 'mfcc_4_max', 'mfcc_5_mean', 'mfcc_5_std', 'mfcc_5_min', 'mfcc_5_max', 'mfcc_6_mean', 'mfcc_6_std', 'mfcc_6_min', 'mfcc_6_max', 'mfcc_7_mean', 'mfcc_7_std', 'mfcc_7_min', 'mfcc_7_max', 'mfcc_8_mean', 'mfcc_8_std', 'mfcc_8_min', 'mfcc_8_max', 'mfcc_9_mean', 'mfcc_9_std', 'mfcc_9_min', 'mfcc_9_max', 'mfcc_10_mean', 'mfcc_10_std', 'mfcc_10_min', 'mfcc_10_max', 'mfcc_11_mean', 'mfcc_11_std', 'mfcc_11_min', 'mfcc_11_max', 'mfcc_12_mean', 'mfcc_12_std', 'mfcc_12_min', 'mfcc_12_max', 'pitch_mean', 'pitch_std', 'pitch_min', 'pitch_max', 'pitch_range', 'energy_mean', 'energy_std', 'energy_min', 'energy_max', 'spectral_centroid_mean', 'spectral_centroid_std', 'zcr_mean', 'zcr_std', 'spectral_rolloff_mean', 'spectral_rolloff_std', 'duration_seconds', 'feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48', 'feat_49', 'feat_50', 'feat_51', 'feat_52', 'feat_53', 'feat_54', 'feat_55', 'feat_56', 'feat_57', 'feat_58', 'feat_59', 'feat_60', 'feat_61', 'feat_62', 'feat_63', 'feat_64', 'feat_65', 'feat_66', 'feat_67', 'feat_68', 'feat_69', 'feat_70', 'feat_71', 'feat_72', 'feat_73', 'feat_74', 'feat_75', 'feat_76', 'feat_77', 'feat_78', 'feat_79', 'feat_80', 'feat_81', 'feat_82', 'feat_83', 'feat_84', 'feat_85', 'feat_86', 'feat_87', 'feat_88', 'feat_89', 'feat_90', 'feat_91', 'feat_92', 'feat_93', 'feat_94', 'feat_95', 'feat_96', 'feat_97', 'feat_98', 'feat_99', 'feat_100', 'feat_101', 'feat_102', 'feat_103', 'feat_104', 'feat_105', 'feat_106', 'feat_107', 'feat_108', 'feat_109', 'feat_110', 'feat_111', 'feat_112', 'feat_113', 'feat_114', 'feat_115', 'feat_116', 'feat_117', 'feat_118', 'feat_119', 'feat_120', 'feat_121', 'feat_122', 'feat_123', 'feat_124', 'feat_125', 'feat_126', 'feat_127', 'feat_128', 'feat_129', 'feat_130', 'feat_131', 'feat_132', 'feat_133', 'feat_134', 'feat_135', 'feat_136', 'feat_137', 'feat_138', 'feat_139', 'feat_140', 'feat_141', 'feat_142', 'feat_143', 'feat_144', 'feat_145', 'feat_146', 'feat_147', 'feat_148', 'feat_149', 'feat_150', 'feat_151', 'feat_152', 'feat_153', 'feat_154', 'feat_155', 'feat_156', 'feat_157', 'feat_158', 'feat_159', 'feat_160', 'feat_161', 'feat_162', 'feat_163', 'feat_164', 'feat_165', 'feat_166', 'feat_167', 'feat_168', 'feat_169', 'feat_170', 'feat_171', 'feat_172', 'feat_173', 'feat_174', 'feat_175', 'feat_176', 'feat_177', 'feat_178', 'feat_179', 'feat_180', 'feat_181', 'feat_182', 'feat_183', 'feat_184', 'feat_185', 'feat_186', 'feat_187', 'feat_188', 'feat_189', 'feat_190', 'feat_191', 'feat_192', 'feat_193', 'feat_194', 'feat_195', 'feat_196', 'feat_197', 'feat_198', 'feat_199', 'feat_200', 'feat_201', 'feat_202', 'feat_203', 'feat_204', 'feat_205', 'feat_206', 'feat_207', 'feat_208', 'feat_209', 'feat_210', 'feat_211', 'feat_212', 'feat_213', 'feat_214', 'feat_215', 'feat_216', 'feat_217', 'feat_218', 'feat_219', 'feat_220', 'feat_221', 'feat_222', 'feat_223', 'feat_224', 'feat_225', 'feat_226', 'feat_227', 'feat_228', 'feat_229', 'feat_230', 'feat_231', 'feat_232', 'feat_233', 'feat_234', 'feat_235', 'feat_236', 'feat_237', 'feat_238', 'feat_239', 'feat_240', 'feat_241', 'feat_242', 'feat_243', 'feat_244', 'feat_245', 'feat_246', 'feat_247', 'feat_248', 'feat_249', 'feat_250', 'feat_251', 'feat_252', 'feat_253', 'feat_254', 'feat_255', 'feat_256', 'feat_257', 'feat_258', 'feat_259', 'feat_260', 'feat_261', 'feat_262', 'feat_263', 'feat_264', 'feat_265', 'feat_266', 'feat_267', 'feat_268', 'feat_269', 'feat_270', 'feat_271', 'feat_272', 'feat_273', 'feat_274', 'feat_275', 'feat_276', 'feat_277', 'feat_278', 'feat_279', 'feat_280', 'feat_281', 'feat_282', 'feat_283', 'feat_284', 'feat_285', 'feat_286', 'feat_287', 'feat_288', 'feat_289', 'feat_290', 'feat_291', 'feat_292', 'feat_293', 'feat_294', 'feat_295', 'feat_296', 'feat_297', 'feat_298', 'feat_299', 'feat_300', 'feat_301', 'feat_302', 'feat_303', 'feat_304', 'feat_305', 'feat_306', 'feat_307', 'feat_308', 'feat_309', 'feat_310', 'feat_311', 'feat_312', 'feat_313', 'feat_314', 'feat_315', 'feat_316', 'feat_317', 'feat_318', 'feat_319', 'feat_320', 'feat_321', 'feat_322', 'feat_323', 'feat_324', 'feat_325', 'feat_326', 'feat_327', 'feat_328', 'feat_329', 'feat_330', 'feat_331', 'feat_332', 'feat_333', 'feat_334', 'feat_335', 'feat_336', 'feat_337', 'feat_338', 'feat_339', 'feat_340', 'feat_341', 'feat_342', 'feat_343', 'feat_344', 'feat_345', 'feat_346', 'feat_347', 'feat_348', 'feat_349', 'feat_350', 'feat_351', 'feat_352', 'feat_353', 'feat_354', 'feat_355', 'feat_356', 'feat_357', 'feat_358', 'feat_359', 'feat_360', 'feat_361', 'feat_362', 'feat_363', 'feat_364', 'feat_365', 'feat_366', 'feat_367', 'feat_368', 'feat_369', 'feat_370', 'feat_371', 'feat_372', 'feat_373', 'feat_374', 'feat_375', 'feat_376', 'feat_377', 'feat_378', 'feat_379', 'feat_380', 'feat_381', 'feat_382', 'feat_383', 'feat_384', 'feat_385', 'feat_386', 'feat_387', 'feat_388', 'feat_389', 'feat_390', 'feat_391', 'feat_392', 'feat_393', 'feat_394', 'feat_395', 'feat_396', 'feat_397', 'feat_398', 'feat_399', 'feat_400', 'feat_401', 'feat_402', 'feat_403', 'feat_404', 'feat_405', 'feat_406', 'feat_407', 'feat_408', 'feat_409', 'feat_410', 'feat_411', 'feat_412', 'feat_413', 'feat_414', 'feat_415', 'feat_416', 'feat_417', 'feat_418', 'feat_419', 'feat_420', 'feat_421', 'feat_422', 'feat_423', 'feat_424', 'feat_425', 'feat_426', 'feat_427', 'feat_428', 'feat_429', 'feat_430', 'feat_431', 'feat_432', 'feat_433', 'feat_434', 'feat_435', 'feat_436', 'feat_437', 'feat_438', 'feat_439', 'feat_440', 'feat_441', 'feat_442', 'feat_443', 'feat_444', 'feat_445', 'feat_446', 'feat_447', 'feat_448', 'feat_449', 'feat_450', 'feat_451', 'feat_452', 'feat_453', 'feat_454', 'feat_455', 'feat_456', 'feat_457', 'feat_458', 'feat_459', 'feat_460', 'feat_461', 'feat_462', 'feat_463', 'feat_464', 'feat_465', 'feat_466', 'feat_467', 'feat_468', 'feat_469', 'feat_470', 'feat_471', 'feat_472', 'feat_473', 'feat_474', 'feat_475', 'feat_476', 'feat_477', 'feat_478', 'feat_479', 'feat_480', 'feat_481', 'feat_482', 'feat_483', 'feat_484', 'feat_485', 'feat_486', 'feat_487', 'feat_488', 'feat_489', 'feat_490', 'feat_491', 'feat_492', 'feat_493', 'feat_494', 'feat_495', 'feat_496', 'feat_497', 'feat_498', 'feat_499', 'feat_500', 'feat_501', 'feat_502', 'feat_503', 'feat_504', 'feat_505', 'feat_506', 'feat_507', 'feat_508', 'feat_509', 'feat_510', 'feat_511', 'feat_512', 'feat_513', 'feat_514', 'feat_515', 'feat_516', 'feat_517', 'feat_518', 'feat_519', 'feat_520', 'feat_521', 'feat_522', 'feat_523', 'feat_524', 'feat_525', 'feat_526', 'feat_527', 'feat_528', 'feat_529', 'feat_530', 'feat_531', 'feat_532', 'feat_533', 'feat_534', 'feat_535', 'feat_536', 'feat_537', 'feat_538', 'feat_539', 'feat_540', 'feat_541', 'feat_542', 'feat_543', 'feat_544', 'feat_545', 'feat_546', 'feat_547', 'feat_548', 'feat_549', 'feat_550', 'feat_551', 'feat_552', 'feat_553', 'feat_554', 'feat_555', 'feat_556', 'feat_557', 'feat_558', 'feat_559', 'feat_560', 'feat_561', 'feat_562', 'feat_563', 'feat_564', 'feat_565', 'feat_566', 'feat_567', 'feat_568', 'feat_569', 'feat_570', 'feat_571', 'feat_572', 'feat_573', 'feat_574', 'feat_575', 'feat_576', 'feat_577', 'feat_578', 'feat_579', 'feat_580', 'feat_581', 'feat_582', 'feat_583', 'feat_584', 'feat_585', 'feat_586', 'feat_587', 'feat_588', 'feat_589', 'feat_590', 'feat_591', 'feat_592', 'feat_593', 'feat_594', 'feat_595', 'feat_596', 'feat_597', 'feat_598', 'feat_599', 'feat_600', 'feat_601', 'feat_602', 'feat_603', 'feat_604', 'feat_605', 'feat_606', 'feat_607', 'feat_608', 'feat_609', 'feat_610', 'feat_611', 'feat_612', 'feat_613', 'feat_614', 'feat_615', 'feat_616', 'feat_617', 'feat_618', 'feat_619', 'feat_620', 'feat_621', 'feat_622', 'feat_623', 'feat_624', 'feat_625', 'feat_626', 'feat_627', 'feat_628', 'feat_629', 'feat_630', 'feat_631', 'feat_632', 'feat_633', 'feat_634', 'feat_635', 'feat_636', 'feat_637', 'feat_638', 'feat_639', 'feat_640', 'feat_641', 'feat_642', 'feat_643', 'feat_644', 'feat_645', 'feat_646', 'feat_647', 'feat_648', 'feat_649', 'feat_650', 'feat_651', 'feat_652', 'feat_653', 'feat_654', 'feat_655', 'feat_656', 'feat_657', 'feat_658', 'feat_659', 'feat_660', 'feat_661', 'feat_662', 'feat_663', 'feat_664', 'feat_665', 'feat_666', 'feat_667', 'feat_668', 'feat_669', 'feat_670', 'feat_671', 'feat_672', 'feat_673', 'feat_674', 'feat_675', 'feat_676', 'feat_677', 'feat_678', 'feat_679', 'feat_680', 'feat_681', 'feat_682', 'feat_683', 'feat_684', 'feat_685', 'feat_686', 'feat_687', 'feat_688', 'feat_689', 'feat_690', 'feat_691', 'feat_692', 'feat_693', 'feat_694', 'feat_695', 'feat_696', 'feat_697', 'feat_698', 'feat_699', 'feat_700', 'feat_701', 'feat_702', 'feat_703', 'feat_704', 'feat_705', 'feat_706', 'feat_707', 'feat_708', 'feat_709', 'feat_710', 'feat_711', 'feat_712', 'feat_713', 'feat_714', 'feat_715', 'feat_716', 'feat_717', 'feat_718', 'feat_719', 'feat_720', 'feat_721', 'feat_722', 'feat_723', 'feat_724', 'feat_725', 'feat_726', 'feat_727', 'feat_728', 'feat_729', 'feat_730', 'feat_731', 'feat_732', 'feat_733', 'feat_734', 'feat_735', 'feat_736', 'feat_737', 'feat_738', 'feat_739', 'feat_740', 'feat_741', 'feat_742', 'feat_743', 'feat_744', 'feat_745', 'feat_746', 'feat_747', 'feat_748', 'feat_749', 'feat_750', 'feat_751', 'feat_752', 'feat_753', 'feat_754', 'feat_755', 'feat_756', 'feat_757', 'feat_758', 'feat_759', 'feat_760', 'feat_761', 'feat_762', 'feat_763', 'feat_764', 'feat_765', 'feat_766', 'feat_767', 'AU01_r_mean', 'AU01_r_std', 'AU01_r_max', 'AU02_r_mean', 'AU02_r_std', 'AU02_r_max', 'AU04_r_mean', 'AU04_r_std', 'AU04_r_max', 'AU05_r_mean', 'AU05_r_std', 'AU05_r_max', 'AU06_r_mean', 'AU06_r_std', 'AU06_r_max', 'AU09_r_mean', 'AU09_r_std', 'AU09_r_max', 'AU10_r_mean', 'AU10_r_std', 'AU10_r_max', 'AU12_r_mean', 'AU12_r_std', 'AU12_r_max', 'AU14_r_mean', 'AU14_r_std', 'AU14_r_max', 'AU15_r_mean', 'AU15_r_std', 'AU15_r_max', 'AU17_r_mean', 'AU17_r_std', 'AU17_r_max', 'AU20_r_mean', 'AU20_r_std', 'AU20_r_max', 'AU25_r_mean', 'AU25_r_std', 'AU25_r_max', 'AU26_r_mean', 'AU26_r_std', 'AU26_r_max', 'AU04_c_mean', 'AU04_c_std', 'AU04_c_max', 'AU12_c_mean', 'AU12_c_std', 'AU12_c_max', 'AU15_c_mean', 'AU15_c_std', 'AU15_c_max', 'AU23_c_mean', 'AU23_c_std', 'AU23_c_max', 'AU28_c_mean', 'AU28_c_std', 'AU28_c_max', 'AU45_c_mean', 'AU45_c_std', 'AU45_c_max', 'Tx_mean', 'Tx_std', 'Ty_mean', 'Ty_std', 'Tz_mean', 'Tz_std', 'Rx_mean', 'Rx_std', 'Ry_mean', 'Ry_std', 'Rz_mean', 'Rz_std', 'confidence_mean', 'confidence_std', 'total_frames', 'session_id', 'PHQ8_Score', 'severity_class_fixed']\n",
      "Preparing modality-specific features...\n",
      "  Audio: 61 features\n",
      "  Text: 0 features\n",
      "  Video: 66 features\n",
      "âœ“ Data normalized successfully (label column: PHQ8_Score)\n",
      "Shapes -> Audio: torch.Size([11, 61]), Text: torch.Size([11, 1]), Video: torch.Size([11, 66])\n",
      "âœ“ Dataloaders ready\n",
      "Using device: cpu\n",
      "âœ“ Model ready (18,852 params)\n",
      "âœ“ Training setup complete\n",
      "Epoch 5/40 - Train MAE: 6.181, Val MAE: 2.490\n",
      "Epoch 10/40 - Train MAE: 5.398, Val MAE: 2.482\n",
      "Epoch 15/40 - Train MAE: 3.192, Val MAE: 2.483\n",
      "Epoch 20/40 - Train MAE: 1.783, Val MAE: 2.917\n",
      "Epoch 25/40 - Train MAE: 1.346, Val MAE: 2.941\n",
      "Epoch 30/40 - Train MAE: 0.587, Val MAE: 3.176\n",
      "Epoch 35/40 - Train MAE: 0.317, Val MAE: 3.166\n",
      "Epoch 40/40 - Train MAE: 0.189, Val MAE: 3.316\n",
      "âœ“ Training done (best Val MAE = 2.442)\n",
      "============================================================\n",
      "ðŸ” Evaluating Best Attention Model on Test Set...\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Test Results:\n",
      "  MAE: 7.9484\n",
      "  RÂ²: -0.9920\n",
      "âœ“ Evaluation complete âœ…\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Import Libraries\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "print(\"âœ“ Libraries imported\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 2: Setup Paths\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\")\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "MODELS_DIR = RESULTS_DIR / \"models\"\n",
    "\n",
    "for d in [DATA_DIR, RESULTS_DIR, MODELS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Paths configured\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 3: Load Data\n",
    "# ============================================================\n",
    "\n",
    "train = pd.read_csv(DATA_DIR / \"train_data.csv\")\n",
    "val = pd.read_csv(DATA_DIR / \"val_data.csv\")\n",
    "test = pd.read_csv(DATA_DIR / \"test_data.csv\")\n",
    "\n",
    "print(f\"âœ“ Data loaded: {len(train)} train, {len(val)} val, {len(test)} test\")\n",
    "print(\"Columns in train data:\", train.columns.tolist())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 4: Prepare Features\n",
    "# ============================================================\n",
    "\n",
    "label_col = \"PHQ8_Score\"\n",
    "\n",
    "# Split features and target\n",
    "X_train = train.drop(columns=[label_col])\n",
    "y_train = train[label_col]\n",
    "\n",
    "X_val = val.drop(columns=[label_col])\n",
    "y_val = val[label_col]\n",
    "\n",
    "X_test = test.drop(columns=[label_col])\n",
    "y_test = test[label_col]\n",
    "\n",
    "# Split by modality (detect columns dynamically)\n",
    "audio_cols = [c for c in X_train.columns if c.startswith(\"audio_\") or c.startswith(\"mfcc_\") or c.startswith(\"pitch\") or c.startswith(\"energy\")]\n",
    "text_cols = [c for c in X_train.columns if c.startswith(\"text_\")]\n",
    "video_cols = [c for c in X_train.columns if c.startswith(\"AU\") or c.startswith(\"Rx\") or c.startswith(\"Ry\") or c.startswith(\"Rz\")]\n",
    "\n",
    "print(\"Preparing modality-specific features...\")\n",
    "print(f\"  Audio: {len(audio_cols)} features\")\n",
    "print(f\"  Text: {len(text_cols)} features\")\n",
    "print(f\"  Video: {len(video_cols)} features\")\n",
    "\n",
    "# Handle missing text features gracefully\n",
    "if not text_cols:\n",
    "    X_train[\"text_dummy\"] = 0\n",
    "    X_val[\"text_dummy\"] = 0\n",
    "    X_test[\"text_dummy\"] = 0\n",
    "    text_cols = [\"text_dummy\"]\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_val = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(f\"âœ“ Data normalized successfully (label column: {label_col})\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 5: Create PyTorch Datasets and Loaders\n",
    "# ============================================================\n",
    "\n",
    "def make_tensor(df, cols):\n",
    "    return torch.tensor(df[cols].values, dtype=torch.float32)\n",
    "\n",
    "train_audio = make_tensor(X_train, audio_cols)\n",
    "train_text = make_tensor(X_train, text_cols)\n",
    "train_video = make_tensor(X_train, video_cols)\n",
    "train_y = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "val_audio = make_tensor(X_val, audio_cols)\n",
    "val_text = make_tensor(X_val, text_cols)\n",
    "val_video = make_tensor(X_val, video_cols)\n",
    "val_y = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "test_audio = make_tensor(X_test, audio_cols)\n",
    "test_text = make_tensor(X_test, text_cols)\n",
    "test_video = make_tensor(X_test, video_cols)\n",
    "test_y = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_audio, train_text, train_video, train_y)\n",
    "val_dataset = TensorDataset(val_audio, val_text, val_video, val_y)\n",
    "test_dataset = TensorDataset(test_audio, test_text, test_video, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"Shapes -> Audio: {train_audio.shape}, Text: {train_text.shape}, Video: {train_video.shape}\")\n",
    "print(\"âœ“ Dataloaders ready\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 6: Define Model\n",
    "# ============================================================\n",
    "\n",
    "class MultiModalAttentionModel(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, video_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.audio_fc = nn.Sequential(nn.Linear(audio_dim, 64), nn.ReLU())\n",
    "        self.text_fc = nn.Sequential(nn.Linear(text_dim, 32), nn.ReLU())\n",
    "        self.video_fc = nn.Sequential(nn.Linear(video_dim, 64), nn.ReLU())\n",
    "\n",
    "        # Attention layers\n",
    "        self.attn_audio = nn.Linear(64, 1)\n",
    "        self.attn_text = nn.Linear(32, 1)\n",
    "        self.attn_video = nn.Linear(64, 1)\n",
    "\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Linear(64 + 32 + 64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio, text, video):\n",
    "        a_out = self.audio_fc(audio)\n",
    "        t_out = self.text_fc(text)\n",
    "        v_out = self.video_fc(video)\n",
    "\n",
    "        a_w = torch.sigmoid(self.attn_audio(a_out).mean(dim=0))\n",
    "        t_w = torch.sigmoid(self.attn_text(t_out).mean(dim=0))\n",
    "        v_w = torch.sigmoid(self.attn_video(v_out).mean(dim=0))\n",
    "\n",
    "        # Normalize weights\n",
    "        weights = torch.softmax(torch.stack([a_w, t_w, v_w]), dim=0)\n",
    "        a_w, t_w, v_w = weights\n",
    "\n",
    "        fused = torch.cat([a_w * a_out, t_w * t_out, v_w * v_out], dim=1)\n",
    "        preds = self.final_fc(fused)\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 7: Training Setup\n",
    "# ============================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MultiModalAttentionModel(len(audio_cols), len(text_cols), len(video_cols)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "print(f\"âœ“ Model ready ({sum(p.numel() for p in model.parameters()):,} params)\")\n",
    "print(\"âœ“ Training setup complete\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 8: Training Loop\n",
    "# ============================================================\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, 41):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        audio, text, video, targets = batch\n",
    "        audio, text, video, targets = audio.to(device), text.to(device), video.to(device), targets.to(device)\n",
    "\n",
    "        preds = model(audio, text, video).squeeze()\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            audio, text, video, targets = batch\n",
    "            audio, text, video, targets = audio.to(device), text.to(device), video.to(device), targets.to(device)\n",
    "            preds = model(audio, text, video).squeeze()\n",
    "            val_loss = criterion(preds, targets)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "    train_mae = np.mean(train_losses)\n",
    "    val_mae = np.mean(val_losses)\n",
    "\n",
    "    if val_mae < best_val:\n",
    "        best_val = val_mae\n",
    "        torch.save(model.state_dict(), MODELS_DIR / \"attention_model_best.pth\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}/40 - Train MAE: {train_mae:.3f}, Val MAE: {val_mae:.3f}\")\n",
    "\n",
    "print(f\"âœ“ Training done (best Val MAE = {best_val:.3f})\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 9: Evaluation\n",
    "# ============================================================\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    preds_all, targets_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for audio, text, video, targets in loader:\n",
    "            audio, text, video, targets = audio.to(device), text.to(device), video.to(device), targets.to(device)\n",
    "            outputs = model(audio, text, video)\n",
    "            preds_all.extend(outputs.squeeze().cpu().numpy())\n",
    "            targets_all.extend(targets.cpu().numpy())\n",
    "\n",
    "    mae = mean_absolute_error(targets_all, preds_all)\n",
    "    r2 = r2_score(targets_all, preds_all)\n",
    "    return mae, r2, preds_all, targets_all\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ” Evaluating Best Attention Model on Test Set...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.load_state_dict(torch.load(MODELS_DIR / \"attention_model_best.pth\", map_location=device))\n",
    "\n",
    "test_mae, test_r2, test_preds, test_targets = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nðŸ“Š Test Results:\")\n",
    "print(f\"  MAE: {test_mae:.4f}\")\n",
    "print(f\"  RÂ²: {test_r2:.4f}\")\n",
    "print(\"âœ“ Evaluation complete âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283cb397-f3cc-46d4-9da9-d19527c73326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
