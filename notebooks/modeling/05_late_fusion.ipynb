{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1ca8e9-7157-4a6b-83e6-90e46fcfba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n",
      "✓ Libraries imported\n",
      "Using device: cpu\n",
      "\n",
      "Loading data...\n",
      "✓ Data loaded\n",
      "\n",
      "Identifying features...\n",
      "  Audio: 68 features\n",
      "  Text: 0 features\n",
      "  Video: 72 features\n",
      "\n",
      "Preparing modality-specific data...\n",
      "✓ Data prepared for all modalities\n",
      "\n",
      "Loading trained models...\n",
      "⚠ Audio model missing, using untrained\n",
      "✓ Text model loaded\n",
      "⚠ Video model missing, using untrained\n",
      "✓ Predictions computed\n",
      "Simple Average Fusion → Test MAE: 10.1560, R²: -1.8379\n",
      "Weighted Fusion → Test MAE: 6.4185, R²: -0.2014\n",
      "Fusion Weights (normalized): [0.5419234  0.45807663]\n",
      "\n",
      "Individual MAEs → Audio: 10.14736270904541, Text: None, Video: 10.164694786071777\n",
      "✓ Late fusion results saved\n",
      "✓ Fusion weights saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTEBOOK 11: Late Fusion - Ensemble of Expert Models\n",
    "\n",
    "SAVE AS: notebooks/modeling/05_late_fusion.ipynb\n",
    "\n",
    "WHAT THIS DOES:\n",
    "- Uses already-trained Audio, Text, Video models\n",
    "- Combines their predictions (weighted average)\n",
    "- Finds optimal weights for fusion\n",
    "- Compares with early fusion\n",
    "- Robust to missing modalities\n",
    "\"\"\"\n",
    "\n",
    "# ========== CELL 1: Import Libraries ==========\n",
    "print(\"Importing libraries...\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========== CELL 2: Load Data ==========\n",
    "print(\"\\nLoading data...\")\n",
    "\n",
    "PROCESSED_DIR = Path(r'C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\data\\processed')\n",
    "MODELS_DIR = Path(r'C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\models\\saved_models')\n",
    "RESULTS_DIR = Path(r'C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\results')\n",
    "\n",
    "train_df = pd.read_csv(PROCESSED_DIR / 'train_data.csv')\n",
    "val_df   = pd.read_csv(PROCESSED_DIR / 'val_data.csv')\n",
    "test_df  = pd.read_csv(PROCESSED_DIR / 'test_data.csv')\n",
    "\n",
    "print(f\"✓ Data loaded\")\n",
    "\n",
    "# ========== CELL 3: Identify Features by Modality ==========\n",
    "print(\"\\nIdentifying features...\")\n",
    "\n",
    "audio_cols = [c for c in train_df.columns if any(x in c for x in ['mfcc','pitch','energy','spectral','zcr','rolloff','duration'])]\n",
    "text_cols  = [c for c in train_df.columns if 'bert' in c.lower() or any(x in c.lower() for x in ['word','positive','negative','question'])]\n",
    "video_cols = [c for c in train_df.columns if 'AU' in c or 'gaze' in c.lower() or any(x in c for x in ['Tx','Ty','Tz','Rx','Ry','Rz'])]\n",
    "\n",
    "print(f\"  Audio: {len(audio_cols)} features\")\n",
    "print(f\"  Text: {len(text_cols)} features\")\n",
    "print(f\"  Video: {len(video_cols)} features\")\n",
    "\n",
    "# ========== CELL 4: Prepare Data ==========\n",
    "print(\"\\nPreparing modality-specific data...\")\n",
    "\n",
    "def prepare_modality_data(df, feature_cols):\n",
    "    if len(feature_cols) == 0:\n",
    "        return None, df['PHQ8_Score'].values, None\n",
    "    X = df[feature_cols].values\n",
    "    y = df['PHQ8_Score'].values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y, scaler\n",
    "\n",
    "X_train_audio, y_train, scaler_audio = prepare_modality_data(train_df, audio_cols)\n",
    "X_val_audio, _, _ = prepare_modality_data(val_df, audio_cols)\n",
    "X_test_audio, _, _ = prepare_modality_data(test_df, audio_cols)\n",
    "\n",
    "if scaler_audio:\n",
    "    X_val_audio = scaler_audio.transform(val_df[audio_cols].values)\n",
    "    X_test_audio = scaler_audio.transform(test_df[audio_cols].values)\n",
    "\n",
    "X_train_text, _, scaler_text = prepare_modality_data(train_df, text_cols)\n",
    "if X_train_text is not None:\n",
    "    X_val_text = scaler_text.transform(val_df[text_cols].values)\n",
    "    X_test_text = scaler_text.transform(test_df[text_cols].values)\n",
    "else:\n",
    "    X_val_text, X_test_text = None, None\n",
    "\n",
    "X_train_video, _, scaler_video = prepare_modality_data(train_df, video_cols)\n",
    "if X_train_video is not None:\n",
    "    X_val_video = scaler_video.transform(val_df[video_cols].values)\n",
    "    X_test_video = scaler_video.transform(test_df[video_cols].values)\n",
    "else:\n",
    "    X_val_video, X_test_video = None, None\n",
    "\n",
    "print(\"✓ Data prepared for all modalities\")\n",
    "\n",
    "# ========== CELL 5: Load or Create Models ==========\n",
    "print(\"\\nLoading trained models...\")\n",
    "\n",
    "def create_simple_model(input_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_size, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(64, 1)\n",
    "    )\n",
    "\n",
    "audio_model = create_simple_model(len(audio_cols)).to(device) if X_train_audio is not None else None\n",
    "text_model  = create_simple_model(len(text_cols)).to(device) if X_train_text is not None else None\n",
    "video_model = create_simple_model(len(video_cols)).to(device) if X_train_video is not None else None\n",
    "\n",
    "try:\n",
    "    if audio_model: audio_model.load_state_dict(torch.load(MODELS_DIR / 'audio_lstm_best.pth', map_location=device))\n",
    "    print(\"✓ Audio model loaded\")\n",
    "except: print(\"⚠ Audio model missing, using untrained\")\n",
    "\n",
    "try:\n",
    "    if text_model: text_model.load_state_dict(torch.load(MODELS_DIR / 'text_bert_best.pth', map_location=device))\n",
    "    print(\"✓ Text model loaded\")\n",
    "except: print(\"⚠ Text model missing, using untrained\")\n",
    "\n",
    "try:\n",
    "    if video_model: video_model.load_state_dict(torch.load(MODELS_DIR / 'video_lstm_best.pth', map_location=device))\n",
    "    print(\"✓ Video model loaded\")\n",
    "except: print(\"⚠ Video model missing, using untrained\")\n",
    "\n",
    "# ========== CELL 6: Predictions ==========\n",
    "def get_predictions_safe(model, X, device):\n",
    "    if X is None or model is None:\n",
    "        return None\n",
    "    model.eval()\n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_tensor)\n",
    "        if len(pred.shape) > 1:\n",
    "            pred = pred.squeeze()\n",
    "    return pred.cpu().numpy()\n",
    "\n",
    "train_pred_audio = get_predictions_safe(audio_model, X_train_audio, device)\n",
    "train_pred_text  = get_predictions_safe(text_model, X_train_text, device)\n",
    "train_pred_video = get_predictions_safe(video_model, X_train_video, device)\n",
    "\n",
    "val_pred_audio = get_predictions_safe(audio_model, X_val_audio, device)\n",
    "val_pred_text  = get_predictions_safe(text_model, X_val_text, device)\n",
    "val_pred_video = get_predictions_safe(video_model, X_val_video, device)\n",
    "\n",
    "test_pred_audio = get_predictions_safe(audio_model, X_test_audio, device)\n",
    "test_pred_text  = get_predictions_safe(text_model, X_test_text, device)\n",
    "test_pred_video = get_predictions_safe(video_model, X_test_video, device)\n",
    "\n",
    "print(\"✓ Predictions computed\")\n",
    "\n",
    "# ========== CELL 7: Simple Average Fusion ==========\n",
    "val_preds  = [p for p in [val_pred_audio, val_pred_text, val_pred_video] if p is not None]\n",
    "test_preds = [p for p in [test_pred_audio, test_pred_text, test_pred_video] if p is not None]\n",
    "\n",
    "val_pred_avg  = np.mean(val_preds, axis=0)\n",
    "test_pred_avg = np.mean(test_preds, axis=0)\n",
    "\n",
    "val_mae_avg  = mean_absolute_error(y_val, val_pred_avg)\n",
    "test_mae_avg = mean_absolute_error(y_test, test_pred_avg)\n",
    "test_rmse_avg = np.sqrt(mean_squared_error(y_test, test_pred_avg))\n",
    "test_r2_avg   = r2_score(y_test, test_pred_avg)\n",
    "\n",
    "print(f\"Simple Average Fusion → Test MAE: {test_mae_avg:.4f}, R²: {test_r2_avg:.4f}\")\n",
    "\n",
    "# ========== CELL 8: Weighted Fusion ==========\n",
    "X_train_fusion = np.column_stack([p for p in [train_pred_audio, train_pred_text, train_pred_video] if p is not None])\n",
    "X_val_fusion   = np.column_stack([p for p in [val_pred_audio, val_pred_text, val_pred_video] if p is not None])\n",
    "X_test_fusion  = np.column_stack([p for p in [test_pred_audio, test_pred_text, test_pred_video] if p is not None])\n",
    "\n",
    "fusion_model = Ridge(alpha=1.0)\n",
    "fusion_model.fit(X_train_fusion, y_train)\n",
    "\n",
    "weights = fusion_model.coef_\n",
    "bias    = fusion_model.intercept_\n",
    "weights_normalized = np.abs(weights) / np.sum(np.abs(weights))\n",
    "\n",
    "val_pred_weighted  = fusion_model.predict(X_val_fusion)\n",
    "test_pred_weighted = fusion_model.predict(X_test_fusion)\n",
    "\n",
    "val_mae_weighted  = mean_absolute_error(y_val, val_pred_weighted)\n",
    "test_mae_weighted = mean_absolute_error(y_test, test_pred_weighted)\n",
    "test_rmse_weighted = np.sqrt(mean_squared_error(y_test, test_pred_weighted))\n",
    "test_r2_weighted   = r2_score(y_test, test_pred_weighted)\n",
    "\n",
    "print(f\"Weighted Fusion → Test MAE: {test_mae_weighted:.4f}, R²: {test_r2_weighted:.4f}\")\n",
    "print(f\"Fusion Weights (normalized): {weights_normalized}\")\n",
    "\n",
    "# ========== CELL 9: Individual Performance ==========\n",
    "def compute_mae(pred, y):\n",
    "    return mean_absolute_error(y, pred) if pred is not None else None\n",
    "\n",
    "audio_mae = compute_mae(test_pred_audio, y_test)\n",
    "text_mae  = compute_mae(test_pred_text, y_test)\n",
    "video_mae = compute_mae(test_pred_video, y_test)\n",
    "\n",
    "print(f\"\\nIndividual MAEs → Audio: {audio_mae}, Text: {text_mae}, Video: {video_mae}\")\n",
    "\n",
    "# ========== CELL 10: Save Results ==========\n",
    "results = {\n",
    "    'model': ['Late Fusion (Average)', 'Late Fusion (Weighted)'],\n",
    "    'method': ['simple_average', 'learned_weights'],\n",
    "    'val_mae': [val_mae_avg, val_mae_weighted],\n",
    "    'test_mae': [test_mae_avg, test_mae_weighted],\n",
    "    'test_rmse': [test_rmse_avg, test_rmse_weighted],\n",
    "    'test_r2': [test_r2_avg, test_r2_weighted]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(RESULTS_DIR / 'metrics' / 'late_fusion_results.csv', index=False)\n",
    "print(f\"✓ Late fusion results saved\")\n",
    "\n",
    "weights_df = pd.DataFrame({\n",
    "    'modality': [m for m,p in zip(['audio','text','video'], [train_pred_audio, train_pred_text, train_pred_video]) if p is not None],\n",
    "    'weight': weights,\n",
    "    'normalized': weights_normalized\n",
    "})\n",
    "weights_df.to_csv(RESULTS_DIR / 'metrics' / 'fusion_weights.csv', index=False)\n",
    "print(f\"✓ Fusion weights saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df587ff8-a9d3-4421-a739-261c67704ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
