{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47cabd94-7768-4c57-b665-86655927d0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n",
      "✓ Libraries imported\n",
      "✓ Paths configured\n",
      "✓ Data loaded: 11 train, 2 val, 3 test\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['phq8'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# CELL 4: Prepare Features\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     51\u001b[39m label_col = \u001b[33m\"\u001b[39m\u001b[33mphq8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m X_train = \u001b[43mtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m y_train = train[label_col]\n\u001b[32m     56\u001b[39m X_val = val.drop(columns=[label_col])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\depression_detection_project\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:5603\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5455\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5456\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5457\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5464\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5465\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5467\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5468\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5601\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5602\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5605\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5607\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5609\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5610\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5611\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\depression_detection_project\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:4810\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4810\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4813\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\depression_detection_project\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:4852\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4850\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4851\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4852\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4853\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4855\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4856\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\depression_detection_project\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['phq8'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Import Libraries\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "print(\"Importing libraries...\")\n",
    "print(\"✓ Libraries imported\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 2: Setup Paths\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\")\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "MODELS_DIR = RESULTS_DIR / \"models\"\n",
    "\n",
    "for d in [DATA_DIR, RESULTS_DIR, MODELS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Paths configured\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 3: Load Data\n",
    "# ============================================================\n",
    "\n",
    "train = pd.read_csv(DATA_DIR / \"train_data.csv\")\n",
    "val = pd.read_csv(DATA_DIR / \"val_data.csv\")\n",
    "test = pd.read_csv(DATA_DIR / \"test_data.csv\")\n",
    "\n",
    "print(f\"✓ Data loaded: {len(train)} train, {len(val)} val, {len(test)} test\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 4: Prepare Features\n",
    "# ============================================================\n",
    "\n",
    "label_col = \"phq8\"\n",
    "\n",
    "X_train = train.drop(columns=[label_col])\n",
    "y_train = train[label_col]\n",
    "\n",
    "X_val = val.drop(columns=[label_col])\n",
    "y_val = val[label_col]\n",
    "\n",
    "X_test = test.drop(columns=[label_col])\n",
    "y_test = test[label_col]\n",
    "\n",
    "# Split by modality (assuming columns start with audio_, text_, video_)\n",
    "audio_cols = [c for c in X_train.columns if c.startswith(\"audio_\")]\n",
    "text_cols = [c for c in X_train.columns if c.startswith(\"text_\")]\n",
    "video_cols = [c for c in X_train.columns if c.startswith(\"video_\")]\n",
    "\n",
    "print(\"Preparing modality-specific features...\")\n",
    "print(f\"  Audio: {len(audio_cols)} features\")\n",
    "print(f\"  Text: {len(text_cols)} features\")\n",
    "print(f\"  Video: {len(video_cols)} features\")\n",
    "\n",
    "# Handle missing text features gracefully\n",
    "if not text_cols:\n",
    "    X_train[\"text_dummy\"] = 0\n",
    "    X_val[\"text_dummy\"] = 0\n",
    "    X_test[\"text_dummy\"] = 0\n",
    "    text_cols = [\"text_dummy\"]\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_val = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(\"✓ Data normalized successfully\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 5: Create PyTorch Datasets and Loaders\n",
    "# ============================================================\n",
    "\n",
    "def make_tensor(df, cols):\n",
    "    return torch.tensor(df[cols].values, dtype=torch.float32)\n",
    "\n",
    "train_audio = make_tensor(X_train, audio_cols)\n",
    "train_text = make_tensor(X_train, text_cols)\n",
    "train_video = make_tensor(X_train, video_cols)\n",
    "train_y = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "val_audio = make_tensor(X_val, audio_cols)\n",
    "val_text = make_tensor(X_val, text_cols)\n",
    "val_video = make_tensor(X_val, video_cols)\n",
    "val_y = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "test_audio = make_tensor(X_test, audio_cols)\n",
    "test_text = make_tensor(X_test, text_cols)\n",
    "test_video = make_tensor(X_test, video_cols)\n",
    "test_y = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_audio, train_text, train_video, train_y)\n",
    "val_dataset = TensorDataset(val_audio, val_text, val_video, val_y)\n",
    "test_dataset = TensorDataset(test_audio, test_text, test_video, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"Shapes -> Audio: {train_audio.shape}, Text: {train_text.shape}, Video: {train_video.shape}\")\n",
    "print(\"✓ Dataloaders ready\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 6: Define Model\n",
    "# ============================================================\n",
    "\n",
    "class MultiModalAttentionModel(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim, video_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.audio_fc = nn.Sequential(nn.Linear(audio_dim, 64), nn.ReLU())\n",
    "        self.text_fc = nn.Sequential(nn.Linear(text_dim, 32), nn.ReLU())\n",
    "        self.video_fc = nn.Sequential(nn.Linear(video_dim, 64), nn.ReLU())\n",
    "\n",
    "        # Attention layers\n",
    "        self.attn_audio = nn.Linear(64, 1)\n",
    "        self.attn_text = nn.Linear(32, 1)\n",
    "        self.attn_video = nn.Linear(64, 1)\n",
    "\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Linear(64 + 32 + 64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio, text, video):\n",
    "        a_out = self.audio_fc(audio)\n",
    "        t_out = self.text_fc(text)\n",
    "        v_out = self.video_fc(video)\n",
    "\n",
    "        a_w = torch.sigmoid(self.attn_audio(a_out).mean(dim=0))\n",
    "        t_w = torch.sigmoid(self.attn_text(t_out).mean(dim=0))\n",
    "        v_w = torch.sigmoid(self.attn_video(v_out).mean(dim=0))\n",
    "\n",
    "        # Normalize weights\n",
    "        weights = torch.softmax(torch.stack([a_w, t_w, v_w]), dim=0)\n",
    "        a_w, t_w, v_w = weights\n",
    "\n",
    "        fused = torch.cat([a_w * a_out, t_w * t_out, v_w * v_out], dim=1)\n",
    "        preds = self.final_fc(fused)\n",
    "\n",
    "        return preds  # returning only preds for simplicity\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 7: Training Setup\n",
    "# ============================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = MultiModalAttentionModel(len(audio_cols), len(text_cols), len(video_cols)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "print(f\"✓ Model ready ({sum(p.numel() for p in model.parameters()):,} params)\")\n",
    "print(\"✓ Training setup complete\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 8: Training Loop\n",
    "# ============================================================\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, 41):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        audio, text, video, targets = batch\n",
    "        audio, text, video, targets = audio.to(device), text.to(device), video.to(device), targets.to(device)\n",
    "\n",
    "        preds = model(audio, text, video).squeeze()\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            audio, text, video, targets = batch\n",
    "            audio, text, video, targets = audio.to(device), text.to(device), video.to(device), targets.to(device)\n",
    "            preds = model(audio, text, video).squeeze()\n",
    "            val_loss = criterion(preds, targets)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "    train_mae = np.mean(train_losses)\n",
    "    val_mae = np.mean(val_losses)\n",
    "\n",
    "    if val_mae < best_val:\n",
    "        best_val = val_mae\n",
    "        torch.save(model.state_dict(), MODELS_DIR / \"attention_model_best.pth\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}/40 - Train MAE: {train_mae:.3f}, Val MAE: {val_mae:.3f}\")\n",
    "\n",
    "print(f\"✓ Training done (best Val MAE = {best_val:.3f})\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CELL 9: Evaluation\n",
    "# ============================================================\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    preds_all, targets_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for audio, text, video, targets in loader:\n",
    "            audio, text, video, targets = audio.to(device), text.to(device), video.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(audio, text, video)\n",
    "            if isinstance(outputs, (tuple, list)):\n",
    "                preds = outputs[0]\n",
    "            else:\n",
    "                preds = outputs\n",
    "\n",
    "            preds_all.extend(preds.squeeze().cpu().numpy())\n",
    "            targets_all.extend(targets.cpu().numpy())\n",
    "\n",
    "    mae = mean_absolute_error(targets_all, preds_all)\n",
    "    r2 = r2_score(targets_all, preds_all)\n",
    "    return mae, r2, preds_all, targets_all\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🔍 Evaluating Best Attention Model on Test Set...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.load_state_dict(torch.load(MODELS_DIR / \"attention_model_best.pth\", map_location=device))\n",
    "\n",
    "test_mae, test_r2, test_preds, test_targets = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\n📊 Test Results:\")\n",
    "print(f\"  MAE: {test_mae:.4f}\")\n",
    "print(f\"  R²: {test_r2:.4f}\")\n",
    "print(\"✓ Evaluation complete ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa812a-eda5-4114-af28-1a5411bb7cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
