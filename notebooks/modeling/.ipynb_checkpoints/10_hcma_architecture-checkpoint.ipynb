{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5025b7a4-d159-4df5-9210-bbc4f84b7460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n",
      "✓ Libraries imported\n",
      "Using device: cpu\n",
      "\n",
      "Loading data...\n",
      "✓ Train: 11, Val: 2, Test: 3\n",
      "\n",
      "Extracting modality-specific features...\n",
      "  Audio: 68 features\n",
      "  Text: 0 features\n",
      "  Video: 72 features\n",
      "⚠ No text features detected. Added dummy column.\n",
      "✓ All modalities prepared and normalized\n",
      "✓ Dataloaders created (batch_size=4)\n",
      "✓ Attention components defined\n",
      "✓ HCMA Model created\n",
      "\n",
      "Model architecture:\n",
      "HCMA(\n",
      "  (audio_proj): Linear(in_features=68, out_features=128, bias=True)\n",
      "  (text_proj): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (video_proj): Linear(in_features=72, out_features=128, bias=True)\n",
      "  (audio_intra_attn): IntraModalAttention(\n",
      "    (query): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (key): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (value): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (text_intra_attn): IntraModalAttention(\n",
      "    (query): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (key): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (value): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (video_intra_attn): IntraModalAttention(\n",
      "    (query): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (key): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (value): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (text_to_video_attn): CrossModalAttention(\n",
      "    (query): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (key): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (value): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (video_to_text_attn): CrossModalAttention(\n",
      "    (query): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (key): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (value): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (audio_to_text_attn): CrossModalAttention(\n",
      "    (query): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (key): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (value): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (text_to_audio_attn): CrossModalAttention(\n",
      "    (query): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (key): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (value): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (audio_to_video_attn): CrossModalAttention(\n",
      "    (query): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (key): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (value): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (video_to_audio_attn): CrossModalAttention(\n",
      "    (query): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (key): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (value): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (fusion_layer1): Linear(in_features=384, out_features=256, bias=True)\n",
      "  (fusion_bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fusion_dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fusion_layer2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fusion_bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fusion_dropout2): Dropout(p=0.3, inplace=False)\n",
      "  (fusion_layer3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fusion_bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fusion_dropout3): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Parameters:\n",
      "  Total: 270,849\n",
      "  Trainable: 270,849\n",
      "\n",
      "✓ Training setup complete\n",
      "  Loss: MSE\n",
      "  Optimizer: AdamW (lr=0.001)\n",
      "  Scheduler: ReduceLROnPlateau\n",
      "✓ Training functions defined\n",
      "\n",
      "============================================================\n",
      "TRAINING HIERARCHICAL CROSS-MODAL ATTENTION (HCMA)\n",
      "============================================================\n",
      "\n",
      "🎯 Target: MAE < 3.0\n",
      "⏰ This will take 30-40 minutes...\n",
      "\n",
      "Epoch 5/60\n",
      "  Train Loss: 49.2484, MAE: 6.2580\n",
      "  Val Loss: 11.9475, MAE: 2.5133, RMSE: 3.4565, R²: -0.9116\n",
      "Epoch 10/60\n",
      "  Train Loss: 45.7444, MAE: 5.9978\n",
      "  Val Loss: 11.5757, MAE: 2.5719, RMSE: 3.4023, R²: -0.8521\n",
      "\n",
      "Early stopping at epoch 14\n",
      "\n",
      "✓ Training complete!\n",
      "Best validation MAE: 2.4939\n",
      "\n",
      "🎉 TARGET ACHIEVED! MAE < 3.0 ✅\n",
      "\n",
      "✓ Model and history saved\n",
      "  Model: C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\models\\saved_models\\hcma_best.pth\n",
      "  Checkpoint: C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\models\\saved_models\\hcma_checkpoint.pth\n",
      "  History: C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\models\\saved_models\\hcma_training_history.pkl\n",
      "\n",
      "🎯 Next: Run Notebook 17 for training visualization and evaluation\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# NOTEBOOK 16: Hierarchical Cross-Modal Attention (HCMA)\n",
    "# ============================================================\n",
    "\n",
    "# ========== CELL 1: Import Libraries ==========\n",
    "print(\"Importing libraries...\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========== CELL 2: Load Data ==========\n",
    "print(\"\\nLoading data...\")\n",
    "\n",
    "PROCESSED_DIR = Path(r'C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\data\\processed')\n",
    "MODELS_DIR = Path(r'C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\models\\saved_models')\n",
    "RESULTS_DIR = Path(r'C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\results')\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS_DIR / 'figures').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(PROCESSED_DIR / 'train_data.csv')\n",
    "val_df = pd.read_csv(PROCESSED_DIR / 'val_data.csv')\n",
    "test_df = pd.read_csv(PROCESSED_DIR / 'test_data.csv')\n",
    "\n",
    "print(f\"✓ Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# ========== CELL 3: Extract Modality Features ==========\n",
    "print(\"\\nExtracting modality-specific features...\")\n",
    "\n",
    "# Identify features by modality\n",
    "audio_cols = [col for col in train_df.columns if any(x in col for x in \n",
    "              ['mfcc', 'pitch', 'energy', 'spectral', 'zcr', 'rolloff', 'duration'])]\n",
    "text_cols = [col for col in train_df.columns if 'bert' in col.lower() or \n",
    "             any(x in col.lower() for x in ['word', 'positive', 'negative', 'question'])]\n",
    "video_cols = [col for col in train_df.columns if 'AU' in col or 'gaze' in col.lower() or \n",
    "              any(x in col for x in ['Tx', 'Ty', 'Tz', 'Rx', 'Ry', 'Rz'])]\n",
    "\n",
    "print(f\"  Audio: {len(audio_cols)} features\")\n",
    "print(f\"  Text: {len(text_cols)} features\")\n",
    "print(f\"  Video: {len(video_cols)} features\")\n",
    "\n",
    "# Handle empty modalities by adding a dummy column\n",
    "if len(text_cols) == 0:\n",
    "    train_df['text_dummy'] = 0.0\n",
    "    val_df['text_dummy'] = 0.0\n",
    "    test_df['text_dummy'] = 0.0\n",
    "    text_cols = ['text_dummy']\n",
    "    print(\"⚠ No text features detected. Added dummy column.\")\n",
    "\n",
    "# Extract and normalize each modality\n",
    "def prepare_modality(df, cols, scaler=None):\n",
    "    X = df[cols].values\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        return X, scaler\n",
    "    else:\n",
    "        X = scaler.transform(X)\n",
    "        return X\n",
    "\n",
    "# Audio\n",
    "X_train_audio, audio_scaler = prepare_modality(train_df, audio_cols)\n",
    "X_val_audio = prepare_modality(val_df, audio_cols, audio_scaler)\n",
    "X_test_audio = prepare_modality(test_df, audio_cols, audio_scaler)\n",
    "\n",
    "# Text\n",
    "X_train_text, text_scaler = prepare_modality(train_df, text_cols)\n",
    "X_val_text = prepare_modality(val_df, text_cols, text_scaler)\n",
    "X_test_text = prepare_modality(test_df, text_cols, text_scaler)\n",
    "\n",
    "# Video\n",
    "X_train_video, video_scaler = prepare_modality(train_df, video_cols)\n",
    "X_val_video = prepare_modality(val_df, video_cols, video_scaler)\n",
    "X_test_video = prepare_modality(test_df, video_cols, video_scaler)\n",
    "\n",
    "# Labels\n",
    "y_train = train_df['PHQ8_Score'].values\n",
    "y_val = val_df['PHQ8_Score'].values\n",
    "y_test = test_df['PHQ8_Score'].values\n",
    "\n",
    "print(\"✓ All modalities prepared and normalized\")\n",
    "\n",
    "# ========== CELL 4: Create Dataset ==========\n",
    "class HCMADataset(Dataset):\n",
    "    \"\"\"Dataset for HCMA with separate modalities\"\"\"\n",
    "    \n",
    "    def __init__(self, audio, text, video, labels):\n",
    "        self.audio = torch.FloatTensor(audio)\n",
    "        self.text = torch.FloatTensor(text)\n",
    "        self.video = torch.FloatTensor(video)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.audio[idx], self.text[idx], self.video[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = HCMADataset(X_train_audio, X_train_text, X_train_video, y_train)\n",
    "val_dataset = HCMADataset(X_val_audio, X_val_text, X_val_video, y_val)\n",
    "test_dataset = HCMADataset(X_test_audio, X_test_text, X_test_video, y_test)\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"✓ Dataloaders created (batch_size={batch_size})\")\n",
    "\n",
    "# ============================================================\n",
    "# Rest of your HCMA model code (Cells 5-10) remains the same\n",
    "# ============================================================\n",
    "\n",
    "# ✅ The critical fix: Empty text modality handled by adding dummy column.\n",
    "\n",
    "\n",
    "# ========== CELL 5: Define HCMA Components ==========\n",
    "\n",
    "class IntraModalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Level 1: Self-attention within a single modality\n",
    "    \n",
    "    Finds which features within the modality are important\n",
    "    Example: In text, \"depressed\" word is more important than \"the\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(IntraModalAttention, self).__init__()\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value = nn.Linear(input_dim, hidden_dim)\n",
    "        self.scale = hidden_dim ** 0.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, features]\n",
    "        \n",
    "        # Add sequence dimension\n",
    "        x = x.unsqueeze(1)  # [batch, 1, features]\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.query(x)  # [batch, 1, hidden]\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # [batch, 1, 1]\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended = torch.matmul(attention_weights, V)  # [batch, 1, hidden]\n",
    "        \n",
    "        return attended.squeeze(1), attention_weights.squeeze()\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Level 2: Cross-attention between two modalities\n",
    "    \n",
    "    One modality (query) attends to another (key, value)\n",
    "    Example: Text asks Video \"do facial expressions match my words?\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, query_dim, kv_dim, hidden_dim=64):\n",
    "        super(CrossModalAttention, self).__init__()\n",
    "        \n",
    "        self.query = nn.Linear(query_dim, hidden_dim)\n",
    "        self.key = nn.Linear(kv_dim, hidden_dim)\n",
    "        self.value = nn.Linear(kv_dim, hidden_dim)\n",
    "        self.scale = hidden_dim ** 0.5\n",
    "        \n",
    "    def forward(self, query_mod, kv_mod):\n",
    "        # query_mod: [batch, query_features]\n",
    "        # kv_mod: [batch, kv_features]\n",
    "        \n",
    "        # Add sequence dimension\n",
    "        query_mod = query_mod.unsqueeze(1)\n",
    "        kv_mod = kv_mod.unsqueeze(1)\n",
    "        \n",
    "        Q = self.query(query_mod)  # [batch, 1, hidden]\n",
    "        K = self.key(kv_mod)\n",
    "        V = self.value(kv_mod)\n",
    "        \n",
    "        # Cross-attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return attended.squeeze(1), attention_weights.squeeze()\n",
    "\n",
    "print(\"✓ Attention components defined\")\n",
    "\n",
    "# ========== CELL 6: Define Complete HCMA Model ==========\n",
    "\n",
    "class HCMA(nn.Module):\n",
    "    \"\"\"\n",
    "    Hierarchical Cross-Modal Attention Network\n",
    "    \n",
    "    Architecture:\n",
    "    1. Project each modality to common dimension\n",
    "    2. Level 1: Intra-modal self-attention\n",
    "    3. Level 2: Cross-modal attention (all pairs)\n",
    "    4. Level 3: Hierarchical fusion\n",
    "    5. Final prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, audio_dim, text_dim, video_dim, hidden_dim=128, output_dim=1):\n",
    "        super(HCMA, self).__init__()\n",
    "        \n",
    "        # === Modality Projection ===\n",
    "        self.audio_proj = nn.Linear(audio_dim, hidden_dim)\n",
    "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "        self.video_proj = nn.Linear(video_dim, hidden_dim)\n",
    "        \n",
    "        # === Level 1: Intra-Modal Attention ===\n",
    "        self.audio_intra_attn = IntraModalAttention(hidden_dim, hidden_dim//2)\n",
    "        self.text_intra_attn = IntraModalAttention(hidden_dim, hidden_dim//2)\n",
    "        self.video_intra_attn = IntraModalAttention(hidden_dim, hidden_dim//2)\n",
    "        \n",
    "        # === Level 2: Cross-Modal Attention ===\n",
    "        # Text <-> Video\n",
    "        self.text_to_video_attn = CrossModalAttention(hidden_dim//2, hidden_dim//2, hidden_dim//4)\n",
    "        self.video_to_text_attn = CrossModalAttention(hidden_dim//2, hidden_dim//2, hidden_dim//4)\n",
    "        \n",
    "        # Audio <-> Text\n",
    "        self.audio_to_text_attn = CrossModalAttention(hidden_dim//2, hidden_dim//2, hidden_dim//4)\n",
    "        self.text_to_audio_attn = CrossModalAttention(hidden_dim//2, hidden_dim//2, hidden_dim//4)\n",
    "        \n",
    "        # Audio <-> Video\n",
    "        self.audio_to_video_attn = CrossModalAttention(hidden_dim//2, hidden_dim//2, hidden_dim//4)\n",
    "        self.video_to_audio_attn = CrossModalAttention(hidden_dim//2, hidden_dim//2, hidden_dim//4)\n",
    "        \n",
    "        # === Level 3: Hierarchical Fusion ===\n",
    "        # Combine: intra-attended + cross-attended features\n",
    "        fusion_dim = (hidden_dim//2) * 3 + (hidden_dim//4) * 6  # 3 intra + 6 cross\n",
    "        \n",
    "        self.fusion_layer1 = nn.Linear(fusion_dim, 256)\n",
    "        self.fusion_bn1 = nn.BatchNorm1d(256)\n",
    "        self.fusion_dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fusion_layer2 = nn.Linear(256, 128)\n",
    "        self.fusion_bn2 = nn.BatchNorm1d(128)\n",
    "        self.fusion_dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fusion_layer3 = nn.Linear(128, 64)\n",
    "        self.fusion_bn3 = nn.BatchNorm1d(64)\n",
    "        self.fusion_dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Output\n",
    "        self.output_layer = nn.Linear(64, output_dim)\n",
    "        \n",
    "    def forward(self, audio, text, video):\n",
    "        # === Step 1: Project to common space ===\n",
    "        audio_h = torch.relu(self.audio_proj(audio))  # [batch, hidden]\n",
    "        text_h = torch.relu(self.text_proj(text))\n",
    "        video_h = torch.relu(self.video_proj(video))\n",
    "        \n",
    "        # === Step 2: Level 1 - Intra-Modal Attention ===\n",
    "        audio_intra, audio_intra_weights = self.audio_intra_attn(audio_h)\n",
    "        text_intra, text_intra_weights = self.text_intra_attn(text_h)\n",
    "        video_intra, video_intra_weights = self.video_intra_attn(video_h)\n",
    "        \n",
    "        # === Step 3: Level 2 - Cross-Modal Attention ===\n",
    "        # Text <-> Video\n",
    "        text_from_video, _ = self.text_to_video_attn(text_intra, video_intra)\n",
    "        video_from_text, _ = self.video_to_text_attn(video_intra, text_intra)\n",
    "        \n",
    "        # Audio <-> Text\n",
    "        audio_from_text, _ = self.audio_to_text_attn(audio_intra, text_intra)\n",
    "        text_from_audio, _ = self.text_to_audio_attn(text_intra, audio_intra)\n",
    "        \n",
    "        # Audio <-> Video\n",
    "        audio_from_video, _ = self.audio_to_video_attn(audio_intra, video_intra)\n",
    "        video_from_audio, _ = self.video_to_audio_attn(video_intra, audio_intra)\n",
    "        \n",
    "        # === Step 4: Level 3 - Hierarchical Fusion ===\n",
    "        # Concatenate all attended representations\n",
    "        fused = torch.cat([\n",
    "            audio_intra, text_intra, video_intra,  # Intra-modal\n",
    "            text_from_video, video_from_text,      # Text-Video cross\n",
    "            audio_from_text, text_from_audio,      # Audio-Text cross\n",
    "            audio_from_video, video_from_audio     # Audio-Video cross\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Fusion layers\n",
    "        out = torch.relu(self.fusion_bn1(self.fusion_layer1(fused)))\n",
    "        out = self.fusion_dropout1(out)\n",
    "        \n",
    "        out = torch.relu(self.fusion_bn2(self.fusion_layer2(out)))\n",
    "        out = self.fusion_dropout2(out)\n",
    "        \n",
    "        out = torch.relu(self.fusion_bn3(self.fusion_layer3(out)))\n",
    "        out = self.fusion_dropout3(out)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.output_layer(out)\n",
    "        \n",
    "        # Return prediction and attention weights for explainability\n",
    "        attention_weights = {\n",
    "            'audio_intra': audio_intra_weights,\n",
    "            'text_intra': text_intra_weights,\n",
    "            'video_intra': video_intra_weights\n",
    "        }\n",
    "        \n",
    "        return output.squeeze(), attention_weights\n",
    "\n",
    "# Create model\n",
    "model = HCMA(\n",
    "    audio_dim=len(audio_cols),\n",
    "    text_dim=len(text_cols),\n",
    "    video_dim=len(video_cols),\n",
    "    hidden_dim=128\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"✓ HCMA Model created\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "\n",
    "# ========== CELL 7: Training Setup ==========\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "print(\"\\n✓ Training setup complete\")\n",
    "print(f\"  Loss: MSE\")\n",
    "print(f\"  Optimizer: AdamW (lr=0.001)\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau\")\n",
    "\n",
    "# ========== CELL 8: Training Functions ==========\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    for audio, text, video, labels in dataloader:\n",
    "        audio, text, video, labels = audio.to(device), text.to(device), video.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(audio, text, video)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predictions.extend(outputs.detach().cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    \n",
    "    return avg_loss, mae\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    all_attention = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for audio, text, video, labels in dataloader:\n",
    "            audio, text, video, labels = audio.to(device), text.to(device), video.to(device), labels.to(device)\n",
    "            \n",
    "            outputs, attn_weights = model(audio, text, video)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "            all_attention.append(attn_weights)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    \n",
    "    return avg_loss, mae, rmse, r2, predictions, targets, all_attention\n",
    "\n",
    "print(\"✓ Training functions defined\")\n",
    "\n",
    "# ========== CELL 9: Train HCMA Model ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING HIERARCHICAL CROSS-MODAL ATTENTION (HCMA)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n🎯 Target: MAE < 3.0\")\n",
    "print(\"⏰ This will take 30-40 minutes...\\n\")\n",
    "\n",
    "num_epochs = 60\n",
    "best_val_mae = float('inf')\n",
    "patience = 12\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_mae = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_mae, val_rmse, val_r2, _, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_maes.append(train_mae)\n",
    "    val_maes.append(val_mae)\n",
    "    \n",
    "    scheduler.step(val_mae)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, MAE: {train_mae:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, R²: {val_r2:.4f}\")\n",
    "    \n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        torch.save(model.state_dict(), MODELS_DIR / 'hcma_best.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_mae': val_mae,\n",
    "        }, MODELS_DIR / 'hcma_checkpoint.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"Best validation MAE: {best_val_mae:.4f}\")\n",
    "\n",
    "if best_val_mae < 3.0:\n",
    "    print(\"\\n🎉 TARGET ACHIEVED! MAE < 3.0 ✅\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Close to target. Current: {best_val_mae:.4f}, Target: < 3.0\")\n",
    "\n",
    "# ========== CELL 10: Save for Next Notebook ==========\n",
    "# Save training history for analysis\n",
    "history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_maes': train_maes,\n",
    "    'val_maes': val_maes,\n",
    "    'best_val_mae': best_val_mae\n",
    "}\n",
    "\n",
    "import pickle\n",
    "with open(MODELS_DIR / 'hcma_training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "print(f\"\\n✓ Model and history saved\")\n",
    "print(f\"  Model: {MODELS_DIR / 'hcma_best.pth'}\")\n",
    "print(f\"  Checkpoint: {MODELS_DIR / 'hcma_checkpoint.pth'}\")\n",
    "print(f\"  History: {MODELS_DIR / 'hcma_training_history.pkl'}\")\n",
    "\n",
    "print(\"\\n🎯 Next: Run Notebook 17 for training visualization and evaluation\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b3ffe5-1dc4-4433-beba-d85e1c373200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
