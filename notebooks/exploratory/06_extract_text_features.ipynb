{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "107aab8f-65f6-40a5-9aa1-2397af6addd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all sessions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:26<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved features to: C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\data\\features\\text_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Initialize BERT model and tokenizer (example)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "def extract_bert_features(text):\n",
    "    \"\"\"Extract BERT features for a given text string.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use CLS token embedding as sentence feature\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "    return cls_embedding\n",
    "\n",
    "def process_session(session_id, base_dir):\n",
    "    transcript_path = base_dir / f\"{session_id}_P\" / f\"{session_id}_TRANSCRIPT.csv\"\n",
    "    try:\n",
    "        # Fix: specify delimiter='\\t' for tab-separated transcript files\n",
    "        df = pd.read_csv(transcript_path, delimiter='\\t')\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {transcript_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Check required columns exist\n",
    "    required_cols = {'start_time', 'stop_time', 'speaker', 'value'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        print(f\"⚠️ Required columns missing in: {transcript_path.name}\")\n",
    "        print(f\"   Found columns: {df.columns.tolist()}\")\n",
    "        return None\n",
    "\n",
    "    # Extract text from participant (assuming 'Ellie' is the interviewer, skip her text)\n",
    "    participant_texts = df[df['speaker'] != 'Ellie']['value'].tolist()\n",
    "    full_text = ' '.join(participant_texts)\n",
    "\n",
    "    # Extract features from combined text\n",
    "    features = extract_bert_features(full_text)\n",
    "\n",
    "    # Return as dict with session_id for dataframe\n",
    "    return {'session_id': session_id, **{f'feat_{i}': features[i] for i in range(len(features))}}\n",
    "\n",
    "def main():\n",
    "    base_dir = Path(r\"C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\data\\raw\\DAIC-WOZ\")\n",
    "    output_file = Path(r\"C:\\Users\\VIJAY BHUSHAN SINGH\\depression_detection_project\\data\\features\\text_features.csv\")\n",
    "\n",
    "    session_ids = [str(i) for i in range(300, 326)]  # Sessions 300 to 325\n",
    "    all_features = []\n",
    "\n",
    "    print(\"Processing all sessions...\")\n",
    "    for session_id in tqdm(session_ids):\n",
    "        features = process_session(session_id, base_dir)\n",
    "        if features is not None:\n",
    "            all_features.append(features)\n",
    "\n",
    "    if all_features:\n",
    "        df_features = pd.DataFrame(all_features)\n",
    "        df_features.sort_values('session_id', inplace=True)\n",
    "        df_features.to_csv(output_file, index=False)\n",
    "        print(f\"✅ Saved features to: {output_file}\")\n",
    "    else:\n",
    "        print(\"⚠ No features extracted. DataFrame is empty.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584f1ef-395d-4e0c-815b-1c2d3be81ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
